{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddharth02003/CS6910_assignments/blob/main/CS6910_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "am22A5hWwqIl"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-0uslSo6wvaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12a4ae8-c5d6-47b5-8ea7-806326ab3109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.7/2.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XHQSqKz-wxOF"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "    return {\n",
        "            'X_train': X_train,\n",
        "            'Y_train': Y_train,\n",
        "            'X_test': X_test,\n",
        "            'Y_test': Y_test\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EJmmLHUiwzR2"
      },
      "outputs": [],
      "source": [
        "class_labels={0:'T-shirt',1:'Trouser',2:'Pullover',3:'Dress',4:'Coat',5:'Sandal',6:'Shirt',7:'Sneaker',8:'Bag',9:'Ankle_Boot'}\n",
        "\n",
        "def label_to_name(label):\n",
        "  if enumerate(label):\n",
        "    l_names=[]\n",
        "    for l in label:\n",
        "      l_names.append(class_labels[l])\n",
        "    return l_names\n",
        "  else:\n",
        "    return class_labels[label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JOXMolAx20p",
        "outputId": "6ad3fe0a-8835-4195-cd30-df0f4d278cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login --relogin\n",
        "entity_name=\"siddharth-s\"\n",
        "\n",
        "project_name=\"FODL_Assignment_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gQ_OCNeEw3h7"
      },
      "outputs": [],
      "source": [
        "def log_images():\n",
        "  images=[]\n",
        "  labels=[]\n",
        "  dataset=load_dataset()\n",
        "  X_train=dataset['X_train']\n",
        "  y_train=dataset['Y_train']\n",
        "  wandb.init(entity=entity_name,project=project_name, name=\"log_images\")\n",
        "  for i in range(100):\n",
        "    if len(labels)==10:\n",
        "      break\n",
        "    if class_labels[y_train[i]] not in labels:\n",
        "      images.append(X_train[i])\n",
        "      labels.append(class_labels[y_train[i]])\n",
        "  wandb.log({\"Images\": [wandb.Image(img, caption=caption) for img, caption in zip(images,labels)]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SMeU4Vsicaoh"
      },
      "outputs": [],
      "source": [
        "class activation(): \n",
        "  def __init__(self,a):\n",
        "    self.a=a\n",
        "\n",
        "  def sigmoid(self,a):\n",
        "    try:\n",
        "      return (1.0/(1.0+np.exp(-a)))\n",
        "    except:\n",
        "      print(\"error\")\n",
        "\n",
        "  def relu(self,a):\n",
        "    return (np.maximum(0,a))\n",
        "\n",
        "  def tanh(self,a):\n",
        "    return np.tanh(a)\n",
        "\n",
        "  def softmax(self,a):\n",
        "    try:\n",
        "      return(np.exp(a)/np.sum(np.exp(a)))\n",
        "    except:\n",
        "      print(\"error\")\n",
        "\n",
        "  def sigmoid_derivative(self,x):\n",
        "    return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "\n",
        "  def tanh_derivative(self,x):\n",
        "    return 1.0 -self.tanh(x)**2\n",
        "\n",
        "  def relu_derivative(self,x):\n",
        "    return 1. * (x>0)\n",
        "     \n",
        "  def softmax_derivative(self,x):\n",
        "    return self.softmax(x) * (1-self.softmax(x))\n",
        "\n",
        "  def derivative(self,x,activation):\n",
        "    if activation == \"sigmoid\":\n",
        "      return self.sigmoid_derivative(x)\n",
        "    elif activation == \"tanh\":\n",
        "      return self.tanh_derivative(x)\n",
        "    elif activation == \"relu\":\n",
        "      return self.relu_derivative(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "55iTx2uL1Z_0"
      },
      "outputs": [],
      "source": [
        "class weights():\n",
        "  def __init__(self,layers):\n",
        "    self.layers=layers\n",
        "\n",
        "  def Xavier(self,layers):\n",
        "    params = {}\n",
        "    for i in range(1,len(layers)):\n",
        "       norm_xav=np.sqrt(6)/np.sqrt(layers[i]+layers[i-1])\n",
        "       params[\"w\"+str(i)]=np.random.randn(layers[i],layers[i-1])*norm_xav\n",
        "       params[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "    return params\n",
        "\n",
        "  def Random(self,layers):\n",
        "    params = {}\n",
        "    for i in range(1,len(layers)):\n",
        "       params[\"w\"+str(i)]=0.01*np.random.randn(layers[i],layers[i-1])\n",
        "       params[\"b\"+str(i)]=0.01*np.random.randn(layers[i],1)\n",
        "    return params\n",
        "\n",
        "  def weight_init(self,init_type = \"random\"):\n",
        "    params={}\n",
        "    if(init_type==\"xavier\"):\n",
        "      params = self.Xavier(self.layers)\n",
        "    elif(init_type==\"random\"):\n",
        "      params = self.Random(self.layers)\n",
        "    else:\n",
        "      print(\"invalid activation function\")\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gP-wp4N2ajjW"
      },
      "outputs": [],
      "source": [
        "def squared_loss(y, y_hat):\n",
        "  error = np.sum(((y - y_hat)**2) / (2 * len(y)))\n",
        "  return error\n",
        "def CrossEntropy(y, y_hat):\n",
        "  error = - np.sum( np.multiply(y , np.log(y_hat)))/len(y)\n",
        "  return error\n",
        "def loss_calc(loss_name, y, y_hat, lambd, layers, parameters):\n",
        "  error=0\n",
        "  if(loss_name == \"squared_loss\"):\n",
        "    error=squared_loss(y, y_hat)\n",
        "  elif(loss_name == \"cross_entropy\"):\n",
        "    error= CrossEntropy(y, y_hat)\n",
        "  regularized_error = 0.0\n",
        "  for i in range(len(layers)-1, 0, -1):\n",
        "    regularized_error += (np.sum(parameters[\"w\"+str(i)]))**2\n",
        "  regularized_error = error + ((lambd/(2*len(y)))*(regularized_error))\n",
        "  return regularized_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CqEVmsE7qx6S"
      },
      "outputs": [],
      "source": [
        "class network():\n",
        "  def __init__(self,X,y,params,active,layers,loss_type):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.params=params\n",
        "    self.active=active\n",
        "    self.layers=layers\n",
        "    self.loss_type=loss_type\n",
        "\n",
        "  def forward_prop(self):\n",
        "   out=copy.deepcopy(self.X)\n",
        "   out=out.reshape(-1,1)\n",
        "   h=[out]\n",
        "   a=[out] \n",
        "\n",
        "   act=activation(a)\n",
        "\n",
        "   if(self.active==\"sigmoid\"):\n",
        "     for i in range(1,len(self.layers)-1):\n",
        "       weights = self.params[\"w\"+str(i)]\n",
        "       biases = self.params[\"b\"+str(i)]\n",
        "\n",
        "       out = np.dot(weights,h[i-1])+biases\n",
        "       a.append(out)\n",
        "       post_a = act.sigmoid(out)\n",
        "       h.append(post_a)\n",
        "  \n",
        "   elif(self.active==\"tanh\"):\n",
        "     for i in range(1,len(self.layers)-1):\n",
        "       weights=self.params[\"w\"+str(i)]\n",
        "       biases=self.params[\"b\"+str(i)]\n",
        "      \n",
        "       out=np.dot(weights,h[i-1])+biases\n",
        "       a.append(out)\n",
        "       post_a=act.tanh(out)\n",
        "       h.append(post_a)\n",
        "  \n",
        "   elif(self.active==\"relu\"):\n",
        "     for i in range(1,len(self.layers)-1):\n",
        "       weights=self.params[\"w\"+str(i)]\n",
        "       biases=self.params[\"b\"+str(i)]\n",
        "      \n",
        "       out=np.dot(weights,h[i-1])+biases\n",
        "       a.append(out)\n",
        "       post_a=act.relu(out)\n",
        "       h.append(post_a)       \n",
        "\n",
        "   else:\n",
        "     print(\"Enter a valid activation function\") \n",
        "   weights=self.params[\"w\"+str(len(self.layers)-1)]\n",
        "   biases=self.params[\"b\"+str(len(self.layers)-1)]\n",
        "  \n",
        "   out=np.dot(weights,h[len(self.layers)-2])+biases\n",
        "   a.append(out)\n",
        "   y_hat=act.softmax(out)\n",
        "   h.append(y_hat)\n",
        "   return h,a,y_hat\n",
        "\n",
        "  def backward_prop(self,y,y_hat,h,a,params,layers):\n",
        "    grad = {}\n",
        "    act=activation(self.active)\n",
        "    if self.loss_type == \"squared_loss\":\n",
        "      grad[\"dh\"+str(len(layers)-1)] = (y_hat - y)\n",
        "      grad[\"da\"+str(len(layers)-1)] = (y_hat - y) * act.softmax_derivative(a[len(layers)-1])\n",
        "\n",
        "    elif self.loss_type == 'cross_entropy':\n",
        "      grad[\"da\"+str(len(layers)-1)] = -(y-y_hat)\n",
        "      grad[\"dh\"+str(len(layers)-1)] = -(y/y_hat)\n",
        "\n",
        "    for i in range(len(layers)-1, 0, -1 ):\n",
        "      grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "      grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "      if i > 1:\n",
        "        grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "        grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], act.derivative(a[i-1],self.active))\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_calc(res,y_t):\n",
        "    acc=0.0   \n",
        "    for x in range(len(res)):\n",
        "      if(res[x].argmax()==y_t[x].argmax()):\n",
        "        acc+=1\n",
        "    acc=acc/len(y_t)\n",
        "    return(acc*100)"
      ],
      "metadata": {
        "id": "xSFLTfMSwsGb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eEc2_5l8n0p2"
      },
      "outputs": [],
      "source": [
        "def run_inference(X,y,parameters,activation,layers):\n",
        "    result = []\n",
        "    for i in range(len(X)):\n",
        "      nn=network(X[i], y[i], parameters, activation, layers,\"squared_loss\")\n",
        "      h,a,y_hat = nn.forward_prop()\n",
        "      y_hat = y_hat.flatten()\n",
        "      result.append(y_hat)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c1bmArXk0T5a"
      },
      "outputs": [],
      "source": [
        "def calculate_grad(X, Y, parameters, activation, layers, loss_function):\n",
        "  grads={}\n",
        "  grads.clear() \n",
        "  for j in range(len(X)):\n",
        "    y = np.reshape(Y[j], (-1,1))\n",
        "\n",
        "    nn=network(X[j], y, parameters, activation, layers, loss_function)\n",
        "    h,a,y_hat = nn.forward_prop()\n",
        "    new_grads = nn.backward_prop(y,y_hat,h,a,parameters,layers)\n",
        "\n",
        "    if j == 0:\n",
        "      grads = copy.deepcopy(new_grads)\n",
        "    else:\n",
        "      for k in range(len(layers)-1,0,-1):\n",
        "        grads[\"dw\"+str(k)] += new_grads[\"dw\"+str(k)]\n",
        "        grads[\"db\"+str(k)] += new_grads[\"db\"+str(k)]\n",
        "  return grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X_train, y_train, eta, max_epochs, layers, mini_batch_size, lambd,loss_function, activation, parameters,optimiser,wandb_log=False):\n",
        "  grads={}\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      \n",
        "      grads.clear()\n",
        "\n",
        "      if str(optimiser) == \"nesterovacc_gd\":\n",
        "        opt=optimiser(grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters)\n",
        "        param_lookahead,update_history=opt.paramlookahead()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      if str(optimiser) == \"nesterovacc_gd\":\n",
        "        grads = calculate_grad(X,Y,param_lookahead,activation,layers,loss_function)\n",
        "      else: \n",
        "        grads = calculate_grad(X,Y,parameters,activation,layers,loss_function)\n",
        "\n",
        "      opt=optimiser(grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters,i)\n",
        "      parameters=opt.get_params()\n",
        "    \n",
        "    #Calculating train loss and accuracies\n",
        "    res = run_inference(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function,y_train,res,lambd,layers,parameters) \n",
        "    train_ac=accuracy_calc(res, y_train)\n",
        "    train_loss.append(train_err)\n",
        "    train_acc.append(train_ac)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = run_inference(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_ac=accuracy_calc(res, y_val)\n",
        "    val_loss.append(val_err)\n",
        "    val_acc.append(val_ac)\n",
        "\n",
        "    if(wandb_log==True):\n",
        "      log_dict = {\"Train_Accuracy\": train_ac, \"Validation_Accuracy\": val_ac, \\\n",
        "                  \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                  \n",
        "      wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_acc, val_acc"
      ],
      "metadata": {
        "id": "LsynyUWa5Y8R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class stochastic_gd():\n",
        "  def __init__(self,grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters,i):\n",
        "    self.grads=grads\n",
        "    self.eta=eta\n",
        "    self.layers=layers\n",
        "    self.mini_batch_size=mini_batch_size\n",
        "    self.parameters=parameters\n",
        "    self.lambd=lambd\n",
        "    self.i=i\n",
        "    \n",
        "  def get_params(self):\n",
        "    for j in range(len(self.layers)-1,0,-1):\n",
        "        self.parameters[\"w\"+str(j)] = (1-((self.eta*self.lambd)/self.mini_batch_size))*self.parameters[\"w\"+str(j)] - (self.eta * self.grads[\"dw\"+str(j)])\n",
        "        self.parameters[\"b\"+str(j)] = self.parameters[\"b\"+str(j)] - (self.eta * self.grads[\"db\"+str(j)])\n",
        "    return self.parameters"
      ],
      "metadata": {
        "id": "xUM1PAe8D0Yt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class momentum_gd():\n",
        "  def __init__(self,grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters,i):\n",
        "    self.grads=grads\n",
        "    self.eta=eta\n",
        "    self.layers=layers\n",
        "    self.mini_batch_size=mini_batch_size\n",
        "    self.parameters=parameters\n",
        "    self.lambd=lambd\n",
        "    self.i=i\n",
        "    \n",
        "  def get_params(self):\n",
        "    update_history={}\n",
        "    if self.i == 0 :\n",
        "        for j in range(len(self.layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = self.eta*self.grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = self.eta*self.grads[\"db\"+str(j)]\n",
        "    else:\n",
        "        for j in range(len(self.layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (self.gamma*update_history[\"w\"+str(j)]) + (self.eta*self.grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (self.gamma*update_history[\"b\"+str(j)]) + (self.eta*self.grads[\"db\"+str(j)])\n",
        "    for j in range(len(self.layers)-1,0,-1):\n",
        "        self.parameters[\"w\"+str(j)] = (1-((self.eta*self.lambd)/self.mini_batch_size))*self.parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        self.parameters[\"b\"+str(j)] = self.parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "    return self.parameters"
      ],
      "metadata": {
        "id": "hwY-M90CHaQ9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class nesterovacc_gd():\n",
        "  def __init__(self,grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters,i):\n",
        "    self.grads=grads\n",
        "    self.eta=eta\n",
        "    self.layers=layers\n",
        "    self.mini_batch_size=mini_batch_size\n",
        "    self.parameters=parameters\n",
        "    self.lambd=lambd\n",
        "    self.i=i\n",
        "\n",
        "  def paramlookahead(self):\n",
        "    update_history={}\n",
        "    if self.i==0:\n",
        "        param_lookahead = copy.deepcopy(self.parameters)\n",
        "    else:\n",
        "        for j in range(len(self.layers)-1, 0, -1):\n",
        "          param_lookahead['w'+str(j)] = self.parameters['w'+str(j)] + (self.gamma*update_history[\"w\"+str(j)])\n",
        "    return param_lookahead,update_history\n",
        "\n",
        "  def get_params(self,update_history):\n",
        "    param_lookahead,update_history=self.paramlookahead()\n",
        "    if self.i == 0 :\n",
        "        for j in range(len(self.layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = self.eta*self.grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = self.eta*self.grads[\"db\"+str(j)]\n",
        "    else:\n",
        "        for j in range(len(self.layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (self.gamma*update_history[\"w\"+str(j)]) + (self.eta*self.grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (self.gamma*update_history[\"b\"+str(j)]) + (self.eta*self.grads[\"db\"+str(j)])\n",
        "    for j in range(len(self.layers)-1,0,-1):\n",
        "        self.parameters[\"w\"+str(j)] = (1-((self.eta*self.lambd)/self.mini_batch_size))*self.parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        self.parameters[\"b\"+str(j)] = self.parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "    return self.parameters\n"
      ],
      "metadata": {
        "id": "0HSpU3xmFPP1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class rmsprop():\n",
        "  def __init__(self,grads, eta, max_epochs,layers,mini_batch_size,lambd,parameters,i):\n",
        "    self.grads=grads\n",
        "    self.eta=eta\n",
        "    self.layers=layers\n",
        "    self.mini_batch_size=mini_batch_size\n",
        "    self.parameters=parameters\n",
        "    self.lambd=lambd\n",
        "    self.i=i\n",
        "    self.beta = 0.9 \n",
        "    self.epsilon=1e-8\n",
        "  \n",
        "  def momenta(self):\n",
        "    update_history={}\n",
        "    v={}\n",
        "    for i in range(len(self.layers)-1,0,-1):\n",
        "      update_history[\"w\"+str(i)]=np.zeros((self.layers[i],self.layers[i-1]))\n",
        "      update_history[\"b\"+str(i)]=np.zeros((self.layers[i],1))\n",
        "    for i in range(len(self.layers)-1,0,-1):\n",
        "      v[\"w\"+str(i)]=np.zeros((self.layers[i],self.layers[i-1]))\n",
        "      v[\"b\"+str(i)]=np.zeros((self.layers[i],1))\n",
        "    return v,update_history\n",
        "     \n",
        "  def get_params(self):\n",
        "    v,update_history=self.momenta()\n",
        "    for iq in range(len(self.layers)-1,0,-1):\n",
        "        v[\"w\"+str(iq)]=self.beta*v[\"w\"+str(iq)]+(1-self.beta)*self.grads[\"dw\"+str(iq)]**2\n",
        "        v[\"b\"+str(iq)]=self.beta*v[\"b\"+str(iq)]+(1-self.beta)*self.grads[\"db\"+str(iq)]**2     \n",
        "        update_history[\"w\"+str(iq)]=self.eta*np.multiply(np.reciprocal(np.sqrt(v[\"w\"+str(iq)]+self.epsilon)),self.grads[\"dw\"+str(iq)])\n",
        "        update_history[\"b\"+str(iq)]=self.eta*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(iq)]+self.epsilon)),self.grads[\"db\"+str(iq)])\n",
        "    for j in range(len(self.layers)-1,0,-1):\n",
        "        self.parameters[\"w\"+str(j)] = (1-((self.eta*self.lambd)/self.mini_batch_size))*self.parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        self.parameters[\"b\"+str(j)] = self.parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "    return self.parameters"
      ],
      "metadata": {
        "id": "HBXMVUIyOjvz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPO_60-cQSWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOKKhqlasNtr"
      },
      "source": [
        "Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-QmsesIsPqS",
        "outputId": "52ce9bde-eab4-4a21-8e09-79531e44ebef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "(train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "num_classes = 10\n",
        "labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "#performing the train-validation split\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "print(\"Shape of each image - 28x28\" )\n",
        "image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "#storing the number of points in each set\n",
        "train_image_count=len(train_x)\n",
        "val_image_count = len(val_x)\n",
        "test_image_count=len(test_x)\n",
        "  \n",
        "X_train=np.zeros((train_image_count,image_shape))\n",
        "X_val=np.zeros((val_image_count,image_shape))\n",
        "X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "# converting the images into grayscale by normalizing\n",
        "for i in range(train_image_count):\n",
        "  X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "for i in range(val_image_count):\n",
        "  X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "for i in range(test_image_count):\n",
        "  X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "y_train = np.zeros((train_y.size, 10))\n",
        "y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "y_val = np.zeros((val_y.size, 10))\n",
        "y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "y_test = np.zeros((test_y.size, 10))\n",
        "y_test[np.arange(test_y.size), test_y] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x-zV1EZfFkkE"
      },
      "outputs": [],
      "source": [
        "def fit(X_train, y_train, layers,wandb_log, learning_rate = 0.0001, initialization_type = \"random\", activation_function = \"sigmoid\", loss_function = \"cross_entropy\", mini_batch_Size = 32, max_epochs = 5, lambd = 0,optimization_function = stochastic_gd): \n",
        "\n",
        "  w=weights(layers)\n",
        "  parameters = w.weight_init(init_type = initialization_type)\n",
        "  parameters, train_acc, val_acc = gradient_descent(X_train, y_train,learning_rate, max_epochs, layers, mini_batch_Size, lambd, loss_function, activation_function, parameters,optimization_function,wandb_log)\n",
        "  \n",
        "  print(\"Training Accuracy:\",train_acc[-1])\n",
        "  print(\"Validation Accuracy:\",val_acc[-1])\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qav7xiFGd3-F"
      },
      "outputs": [],
      "source": [
        "# layers=[784,32,10]\n",
        "# wandb.init(entity_name,project_name)\n",
        "# wandb_log=True\n",
        "# fit(X_train, y_train, layers,wandb_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2CE-wU16tAGm"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "  #Declaring the dictionary with the default hyperparameters\n",
        "  config_defaults = {\n",
        "      'number_hidden_layers': 2,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'sigmoid',\n",
        "      'mini_batch_size' : 64,\n",
        "      'max_epochs': 5,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': \"adam\"\n",
        "  }\n",
        "\n",
        "  # Initializing the wandb run\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  # Constructing the layers i.e., the architecture of our neural network\n",
        "  layers = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layers = layers + [config.number_neurons]\n",
        "  layers  = layers + [10]\n",
        "\n",
        "  #Collecting all the hyperparameters from the wandb run\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = \"cross_entropy\"\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  opt_fun = config.optimization_function\n",
        "  hidden_layers=config.number_hidden_layers\n",
        "\n",
        "  #Calling the respective hyperparameters\n",
        "  if opt_fun == \"adam\":\n",
        "    optimization_function = adam\n",
        "  elif opt_fun == \"nadam\":\n",
        "    optimization_function = nadam\n",
        "  elif opt_fun == \"stochastic_gd\":\n",
        "    optimization_function = stochastic_gd\n",
        "  elif opt_fun == \"momentum_gd\":\n",
        "    optimization_function = momentum_gd\n",
        "  elif opt_fun == \"nesterov-acc_gd\":\n",
        "    optimization_function = nesterovacc_gd\n",
        "  elif opt_fun == \"rmsprop\":\n",
        "    optimization_function = rmsprop\n",
        "  else:\n",
        "    print(\"Wrong optimization function\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "  #Forming meaningful run name using the hyperparameters\n",
        "  name_run = str(hidden_layers) + \"_\" + initialization_type[0] + \"_\" + \\\n",
        "  activation_function[0] + \"_\" + str(learning_rate) \n",
        "  \"_\" + opt_fun[:4]\n",
        "\n",
        "  wandb.run.name = name_run\n",
        "  wandb_log=True\n",
        "  \n",
        "  parameters = fit(X_train, y_train, layers,wandb_log, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, 5,0,optimization_function)\n",
        "  \n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "C1k4ufNRtHHt"
      },
      "outputs": [],
      "source": [
        "def do_sweep(entity_name,project_name):\n",
        "\n",
        "  #Using bayes method for hyperparameter sweeps to curb the unnecessary configurations\n",
        "  hyperparameters = {\n",
        "      \"learning_rate\":{\n",
        "        'values': [0.001, 0.0001]\n",
        "      },\n",
        "\n",
        "      \"number_hidden_layers\": {\n",
        "          'values' : [3, 4, 5]\n",
        "      },\n",
        "\n",
        "      \"number_neurons\": {\n",
        "        'values': [32, 64, 128]\n",
        "      },\n",
        "\n",
        "      \"initialization_type\": {\n",
        "          'values' : [\"xavier\", \"random\"]\n",
        "      },\n",
        "\n",
        "      \"activation_function\": {\n",
        "          'values': [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "      },\n",
        "\n",
        "      \"mini_batch_size\": {\n",
        "          'values': [16,32,64,128]\n",
        "      },\n",
        "\n",
        "      \"max_epochs\": {\n",
        "          'values': [5, 10, 20]\n",
        "      },\n",
        "\n",
        "      \"lambd\": {\n",
        "          'values': [0, 0.0005, 0.5]\n",
        "      },\n",
        "\n",
        "      \"optimization_function\": {\n",
        "          'values': [\"stochastic_gd\",\"momentum_gd\",\"nesterovacc_gd\"]\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "  #Using bayes method for hyperparameter sweeps to curb the unnecessary configurations\n",
        "  sweep_config = {\n",
        "      'method' : 'bayes',\n",
        "      'metric' :{\n",
        "          'name': 'Validation_Accuracy',\n",
        "          'goal': 'maximize'\n",
        "      },\n",
        "      'parameters': hyperparameters\n",
        "  }\n",
        "\n",
        "  sweep_id = wandb.sweep(sweep_config, entity=entity_name, project=project_name)\n",
        "  wandb.agent(sweep_id, train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c0d950b9ec134053981a828c323c44ee",
            "425ed20f0af44f32a09397bec7c8b49c",
            "9b3e9f4a1fcf40358da057726c3f2bdc",
            "b558fafb176141a0ae3fc3098397074f",
            "5164d95451d0470199e451c406589942",
            "45de0eb3a00e4039b333e0389e64ac7b",
            "d0e3ea216c2c4cbe86c341466fe44d1a",
            "4e2b642ffb474176aeaba00def450838"
          ]
        },
        "id": "jows10uAtPGK",
        "outputId": "9c3b7035-3056-438c-9f36-f3ab2c51ae82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: tbugiiwc\n",
            "Sweep URL: https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ymz1h82u with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization_type: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambd: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimization_function: stochastic_gd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mna20b064\u001b[0m (\u001b[33msiddharth-s\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230316_205446-ymz1h82u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/ymz1h82u' target=\"_blank\">prime-sweep-1</a></strong> to <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/ymz1h82u' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/ymz1h82u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [02:34<00:00, 30.94s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 10.05\n",
            "Validation Accuracy: 9.55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0d950b9ec134053981a828c323c44ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train_Accuracy</td><td>▁▁▁██</td></tr><tr><td>Train_Loss</td><td>█▃▁▁▁</td></tr><tr><td>Validation_Accuracy</td><td>███▁▁</td></tr><tr><td>Validation_loss</td><td>█▄▂▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train_Accuracy</td><td>10.05</td></tr><tr><td>Train_Loss</td><td>2.30258</td></tr><tr><td>Validation_Accuracy</td><td>9.55</td></tr><tr><td>Validation_loss</td><td>2.30274</td></tr><tr><td>epoch</td><td>4</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">prime-sweep-1</strong> at: <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/ymz1h82u' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/ymz1h82u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230316_205446-ymz1h82u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w5oc79hw with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization_type: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambd: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_hidden_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimization_function: nesterovacc_gd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230316_205733-w5oc79hw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/w5oc79hw' target=\"_blank\">decent-sweep-2</a></strong> to <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/w5oc79hw' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/w5oc79hw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong optimization function\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">decent-sweep-2</strong> at: <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/w5oc79hw' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/w5oc79hw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230316_205733-w5oc79hw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run w5oc79hw errored: UnboundLocalError(\"local variable 'optimization_function' referenced before assignment\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run w5oc79hw errored: UnboundLocalError(\"local variable 'optimization_function' referenced before assignment\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: km2rl5te with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization_type: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambd: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_hidden_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimization_function: stochastic_gd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230316_205743-km2rl5te</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/km2rl5te' target=\"_blank\">fresh-sweep-3</a></strong> to <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/sweeps/tbugiiwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/km2rl5te' target=\"_blank\">https://wandb.ai/siddharth-s/FODL_Assignment_1/runs/km2rl5te</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "do_sweep(entity_name,project_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0d950b9ec134053981a828c323c44ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_425ed20f0af44f32a09397bec7c8b49c",
              "IPY_MODEL_9b3e9f4a1fcf40358da057726c3f2bdc"
            ],
            "layout": "IPY_MODEL_b558fafb176141a0ae3fc3098397074f"
          }
        },
        "425ed20f0af44f32a09397bec7c8b49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5164d95451d0470199e451c406589942",
            "placeholder": "​",
            "style": "IPY_MODEL_45de0eb3a00e4039b333e0389e64ac7b",
            "value": "0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "9b3e9f4a1fcf40358da057726c3f2bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0e3ea216c2c4cbe86c341466fe44d1a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2b642ffb474176aeaba00def450838",
            "value": 0.07311621966794381
          }
        },
        "b558fafb176141a0ae3fc3098397074f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5164d95451d0470199e451c406589942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45de0eb3a00e4039b333e0389e64ac7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0e3ea216c2c4cbe86c341466fe44d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2b642ffb474176aeaba00def450838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}